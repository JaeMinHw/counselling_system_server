import numpy as np
import librosa
from tensorflow.keras.models import load_model

# âœ… ëª¨ë¸ê³¼ í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
MODEL_PATH = "1_best_model.h5"
TEST_WAV = "C:/Users/User/Desktop/server/t2.wav"  # ì˜ˆì¸¡í•  .wav íŒŒì¼ ê²½ë¡œ

# âœ… ê°ì • ë¼ë²¨ ì •ì˜ (ëª¨ë¸ í•™ìŠµ ì‹œ ì‚¬ìš©í–ˆë˜ ìˆœì„œì™€ ì¼ì¹˜í•˜ë„ë¡)
EMOTIONS = {
    '1': 'ê¸°ì¨',
    '2': 'ìŠ¬í””',
    '3': 'ë¶„ë…¸',
    '4': 'ë¶ˆì•ˆ',
    '5': 'ìƒì²˜',
    '6': 'ë‹¹í™©',
    '7': 'ì¤‘ë¦½'
}

# âœ… EMOTIONSë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë ¬í•´ì„œ ëª¨ë¸ ì¶œë ¥ ì¸ë±ìŠ¤ì™€ ë§¤í•‘
# í‚¤ ì •ë ¬ ê²°ê³¼: ['1', '2', ..., '7'] â†’ ['ê¸°ì¨', 'ìŠ¬í””', ..., 'ì¤‘ë¦½']
EMOTION_LABELS = [EMOTIONS[k] for k in sorted(EMOTIONS.keys())]

# âœ… íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜ (json ì—†ì´)
def extract_features_for_test(wav_path, max_pad_len=300):
    try:
        audio, sample_rate = librosa.load(wav_path, sr=None, res_type='kaiser_fast')

        if len(audio) < 2048:
            audio = np.pad(audio, (0, 2048 - len(audio)), 'constant')

        # íŠ¹ì„± ì¶”ì¶œ
        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)
        chroma = librosa.feature.chroma_stft(y=audio, sr=sample_rate)
        mel = librosa.feature.melspectrogram(y=audio, sr=sample_rate)
        contrast = librosa.feature.spectral_contrast(y=audio, sr=sample_rate)
        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(audio), sr=sample_rate)
        zcr = librosa.feature.zero_crossing_rate(audio)
        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sample_rate)
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate)
        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sample_rate)

        # íŠ¹ì„± ë³‘í•© ë° ì •ê·œí™”
        feature_list = [mfccs, chroma, mel, contrast, tonnetz, zcr, spectral_centroid, spectral_rolloff, spectral_bandwidth]
        min_frames = min(f.shape[1] for f in feature_list)
        feature_list = [f[:, :min_frames] for f in feature_list]
        features = np.vstack(feature_list).T

        # íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°
        if features.shape[0] < max_pad_len:
            features = np.pad(features, ((0, max_pad_len - features.shape[0]), (0, 0)), mode='constant')
        else:
            features = features[:max_pad_len, :]

        return features
    except Exception as e:
        print(f"âŒ íŠ¹ì§• ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# âœ… ê°ì • ì˜ˆì¸¡ í•¨ìˆ˜
def predict_emotion(wav_path):
    model = load_model(MODEL_PATH)
    features = extract_features_for_test(wav_path)
    if features is None:
        print("âŒ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨. ì˜ˆì¸¡ ì¤‘ë‹¨.")
        return

    # ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
    features = np.expand_dims(features, axis=0)

    # ì˜ˆì¸¡
    prediction = model.predict(features)
    predicted_index = np.argmax(prediction)
    
    # ì˜ˆì¸¡ëœ ê°ì •
    if predicted_index < len(EMOTION_LABELS):
        predicted_emotion = EMOTION_LABELS[predicted_index]
    else:
        predicted_emotion = "ì•Œ ìˆ˜ ì—†ìŒ"

    print("ğŸ§ ì˜ˆì¸¡ëœ ê°ì •:", predicted_emotion)
    print("ğŸ“Š í™•ë¥  ë¶„í¬:")
    for i, prob in enumerate(prediction[0]):
        label = EMOTION_LABELS[i] if i < len(EMOTION_LABELS) else f"ë¼ë²¨{i}"
        print(f"  {label}: {prob:.4f}")

# âœ… ì‹¤í–‰
predict_emotion(TEST_WAV)
